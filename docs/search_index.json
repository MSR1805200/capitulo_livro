[["index.html", "CAPITULO 1 - APRENDIZADO DE MÁQUINA RESUMO", " CAPITULO 1 - APRENDIZADO DE MÁQUINA Mateus Silva Rocha 2022-06-03 RESUMO O aprendizado de máquina se tornou uma tendência tecnológica muito polular desde o início do século. Com ela, implementamos algoritmos inteligêntes capazes de aprender dado um conjunto de dados e assim generalizar para novos dados então desconhecidos. Neste capítulo, introduziremos o conceito de aprendizado de máquina, os principais paradigmas presentes nessa área, a formalização e notação matemática, os conjunto de passos para a realização de um projeto de aprendizado de máquina, os principais algoritmos existentes e práticas computacionais utilizando as linguagens de programação R e Python. "],["introdução.html", "INTRODUÇÃO", " INTRODUÇÃO O aprendizado de máquina pode ser entendido com um subcampo da inteligência artificial que tem como o objetivo de dá as máquinas a capacidade de aprender, dado um conjunto de dados. A enfase está nos dados, isso significa que um algoritmo de aprendizado de máquina aprende pelo processo indutivo isto é, aprende por exemplos já consolidados préviamente. O conceito de aprendizado de máquina é mais antiga do que aparenta ser. Nos anos 50, Alan Turing em seu artigo intitulado Computing Machinery and Intelligence já tinha nos mostrando um vislumbre de como seria os sistemas inteligêntes que são utilizados hoje em dia, onde ensinariamos a aprender como se ensinariamos a uma criança. Esse conceito foi cada vez mais convergindo para o que conhecemos hoje em dia como aprendizado de máquina. Hoje em dia, com o advento da internet e do avanço das tecnologias de hardware e software, conseguimos coletar, processar e analisar as quantidade massivas de dados que a internet proporciona. Em consequência desse processo, temos uma valorização de especialistas que conseguem manipular dados brutos e transformar em conhecimento útil, como por exemplos, analistas e ciêntistas de dados. Estes profissionais utilizam ferramentas sofisticadas que tratam os dados provindos da internet. Dois exemplos populares são as linguagens de programação R e Python. "],["principais-paradigmas-de-aprendizado.html", "PRINCIPAIS PARADIGMAS DE APRENDIZADO Supervisionado Não supervisionado Aprendizado por reforço", " PRINCIPAIS PARADIGMAS DE APRENDIZADO Supervisionado O aprendizado supervisionado consiste em um conjunto de dados rotulados que serão usados para alimentar o algoritmo de aprendizado de máquina, por exemplo uma base de dados para saber se um paciente possui uma determinada doença ou não. Esse tipo de aprendizado supervisionado, onde temos os rótulos em categorias damos o nome de classificação. Quando os rótulos são contínuos como o preço de uma casa, damos o nome de regressão. Esse paradigma em específico é bem popular em vários campos da ciência, por exemplo na deteção de câncer de pele na área da saúde ou doenças em geral como o covid-19. Já no campo financeiro, é utilizado modelos que predizem dados contínuos em relação ao faturamento da empresa.No campo linguistico, são implementados classificadores pra detecção de emoções humanas ou até mesmo para entender conversas, como por exemplo o que é conhecido como agentes conversacionais ou chatbot, que usam exemplos de conversas entre seres humanos para poderem interagirem. Não supervisionado No paradigma de aprendizado não supervisionado não temos um conjunto de dados com rótulos pré-determinados. Nesse caso, é o próprio algoritmo de aprendizado de máquina que o identifica. Nesse caso temos o que é conhecido como agrupamento ou clusterização, mas o algoritmo pode ser utilizado para outros objetivos, tais como a detecção de anomalias ou redução de dimensionalidade. Esse tipo específico de algoritmo é bastante utilizado nas empresas para descobrir categorias(segmentos) de clientes ou pra detectar anomalias de compras de cartão de crédito para evitar fraudes, pois caso uma compra saida do padrão normal de compras de um usuário, o algoritmo vai detectar e entrarão em contato. Aprendizado por reforço O paradigma de aprendizado por reforço funciona de uma forma completamente diferente dos outros paradigmas. Nele, temos um agente que, dados o estado atual e um ambiente, seleciona a melhor ação a se tomar para maximizar sua recompensa, caso tome uma ação considerada como ruim, terá uma punição. Suas aplicações são bem frequentes nos noticiários, como por exemplo o robô DeepBlue da IBM que consegue jogar xadrez como um enxadrista profissional e venceu o famoso jogador de xadrez Garry Kasparov. "],["passos-para-a-realização-de-um-projeto-de-aprendizado-de-máquina.html", "PASSOS PARA A REALIZAÇÃO DE UM PROJETO DE APRENDIZADO DE MÁQUINA Entendimento do problema e coleta de dados Divisão do conjunto de dados Análise exploratória Imputação e transformação dos dados Comparação de modelos Aprimoramento do modelo Teste final", " PASSOS PARA A REALIZAÇÃO DE UM PROJETO DE APRENDIZADO DE MÁQUINA O processo de implementação de um modelo de aprendizado de máquina segue um conjunto de passos sistematizados que envolve desde a coleta dos dados propriamente ditos até a avaliação final do modelo. Entendimento do problema e coleta de dados Para começar a realizar um projeto ponta a ponta de aprendizado de máquina, deve-se ter em mente que tipo de problema se quer resolver e quais objetivos que precisam ser atingidos. Não se pode trabalhar com a implementação sem conhecer o problema que se quer resolver, pois impactará na forma que você desenvolve o projeto, quais modelos utilizar ou qual métrica será mais adequada. Definindo o problema e os objetivos, o proximo passso é coletar os dados. Dependendo do problema, os dados já estarão prontos para serem usados. Um exemplo seria se quisesse predizer quais pessoas sobreviveram no naufrágio do Titanic, nesse caso já temos uma plataforma online que nos dá o conjunto de dados de antemão denominado Kaggle. Caso não encontre nenhum banco de dados nas plataformas online, uma possível alternativa seria realizar um processo de amostragem. Vale lembrar que a coleta possui papel central no projeto, pois se de algum modo estiver enviesada, esse viés reflitará no modelo final. Divisão do conjunto de dados Feito a coleta, antes de realizar qualquer análise, deve ser feito uma divisão na base de dados. A técnica mais utilizada nesse primeiro momento chama-se holdout, onde é escolhido uma proporção \\(p\\) para o conjunto de treino e seu complementar \\(1 - p\\) será usado no conjunto de teste. Geralmente é utilizado \\(80\\%\\) dos dados totais para o treinamento e \\(20\\%\\) para o teste do modelo. O conjunto de treinamento é bastante usado em todo o processo na criação do projeto. É nele que realizamos as análises e alimentamos os modelos. Já o conjunto de teste é utilizado apenas uma única vez na parte final do projeto com o objetivo de estimar o erro fora da amostra ou também chamado de erro de generalização. Análise exploratória Na análise exploratória é onde é feito as análises descritivas no conjunto de dados de treino, como por exemplo: medidas de tendências central, grau de disparidade, etc. É feito gráficos para observar o comportamento e as relações entre variáveis. É nele também onde podemos combinar variáveis para criar uma nova que talvez ajude o modelo. Esse processo tem o nome de feature engineering ou engenharia de recursos. Imputação e transformação dos dados Feito o processo de análise exploratória, o proximo passo será imputar e transformar o conjunto de dados de treino para então usá-lo para escolher o modelo. O processo de imputação consiste em preencher valores ausentes em um conjunto de dados. Geralmente é utilizado medidas de tendência central como média e mediana para valores numéricos e a moda para valores categóricos. Uma possível alternativa seria também usar um modelo de aprendizado de máquina para preencher os valores, como por exemplo o método de vizinhos mais próximos. Na etapa de transformação, realizamos processos de conversão de variáveis categóricas para numéricas, como por exemplo o one-hot-encodidng. Ele produz colunas binárias para cada categoria em uma coluna e adiciona o valor \\(1\\) caso cada instância possua a categoria da coluna binária. Outros dois processos que são amplamente utilizados são a normalização e a padronização. A normalização deixa os valores das colunas situados entre o intevalo de \\(0\\) a \\(1\\), já a padronização redimenciona os valores para terem média \\(0\\) e variancia \\(1\\). Comparação de modelos Feito todo o processo de imputação e transoformação, chega a hora de escolher o modelo de aprendizado de máquina. Essa etapa pode ser realizada refazendo a técnica de holdout dividindo os dados de treino em dois: treino e validação. Geralmente é feita outra técnica denominada validação cruzada. Nela dividimos os dados em \\(K\\) pacotes também chamados de folds e em cada iteração, um pacote será utilizado para teste e o resto para treino. Esse método é mais vantajoso que a técnica de hodout, pois usa todo o conjunto de dados de treino, invés de separa-lo em dois, porém é muito custoso em termos computacionais. Cada modelo será avalidado de acordo com as métricas pré-estabelecidas no início e será escolhido o modelo que tiver melhor performance tanto nas métricas como também na velocidade de treino. Aprimoramento do modelo Escolhido o modelo, o próximo passo será aprimora-lo escolhendo os melhores valores de seues hiperparâmetros. Um hiperparâmetro não é estimado, mas sim escolhido pelo usuário, portanto se fazem técnicas de busca desses parâmetros como o que é conhecido como pesquisa em grade ou grid search. A pesquisa em grade usa uma grade pré-estabelecida de valores de hiperparâmetros e busca os melhores baseando-se em sua métrica. Há também o random search que faz o mesmo processo, porém não utiliza todos os valores, aleatorizando o processo. Teste final Realizando todo o processo de aprimoramento, o passo final é testar na base de dados de teste. Antes de testar, deve-se realizar o processo de imputação e transformação dos dados antes utilizados no treinamento. Feito isso basta alimentar o modelo aprimorado com a base de teste e testa-lo. Obtendo um desempemho satisfatório, o modelo está apto a realizar a tarefa que foi específicada na problemática da situação, como por exemplo subi-lo na nuvem, isto é coloca-lo online para outras pessoas utilizarem. "],["algoritmos-preditivos.html", "ALGORITMOS PREDITIVOS K-vizinhos mais próximos Árvore de decisão Suport Vector Machine(Máquinas de vetores de suporte) Regressão Linear Regressão Logística Naive Bayes Modelos Ensemble", " ALGORITMOS PREDITIVOS K-vizinhos mais próximos O algoritmo de K-vizinhos mais próximos ou KNN(k-nearest neighbors) é um tipo de algoritmo de aprendizado de máquina baseado em exemplos ou instâncias. Isso significa que, invés de estimar algum parâmetro para realizar predições cada vez mais precisas, ele realiza um processo de “memorização” baseado em algum critério de similaridade entre as instâncias do conjunto de dados. O seu funcionamento consiste em escolher um número K de vizinhos mais próximos para realizar a predição.Definido a quantidade, o algoritmo irá percorrer todo o conjunto de dados procurando os k mais próximos. Por fim, caso o algoritmo seja usado para classificação, irá ser feito pela classe mais frequente, ou seja voto majoritário, caso seja usado para regressão, será computado a média dos k vizinhos mais próximos: Algoritmo 1: K vizinhos mais próximos para \\(K = 1\\) Inicialize \\(D\\): \\(D = \\{(x^{(1)}, y^{(1)}),...,(x^{(n)},y^{(n)})\\}\\) Inicialize \\(x^{(P)}\\): Sendo \\(x^{(P)}\\) como a instância para predição. Inicialize \\(d\\) : \\(d(x^{(i)},x^{(p)})\\) como a medida de similaridade. instancia_perto = \\(none\\) distancia_mais_proxima = \\(\\infty\\) rotulo_perto = \\(none\\) Para \\(i = 1,...,n\\) faça:    distancia = \\(d(x^{(i)},x^{(p)})\\)    Se distancia \\(\\leq\\) distancia_mais_proxima então:        distancia_mais_proxima = distancia        instancia_perto = \\(x^{(i)}\\)        rotulo_perto = \\(y^{(i)}\\) retorne rotulo_perto Observe abaixo exemplos de medidas de similaridade comumente utilizas no algoritmo: \\[\\begin{equation} d_{E}(\\vec{x}, \\vec{y}) = \\sqrt{\\sum_{i = 1}^{n}(\\vec{x_{i}} -\\vec{y_{i}})^{2}} \\tag{1} \\end{equation}\\] \\[\\begin{equation} d_{M}(\\vec{x}, \\vec{y}) = \\sum_{i = 1}^{n} | \\vec{x_{i}} - \\vec{y_{i}}| \\tag{2} \\end{equation}\\] Árvore de decisão A árvore de decisão é um método de aprendizado simbólico que é usado tanto para a classificação quanto para a regressão. O funcionamento de uma árvore de decisão em um problema de classificação consiste em criar partições em seu conjunto de dados de modo que deixe os dados homogêneos em relação a seus rótulos. Primeiro defini-se atributos que sirvam como critérios para divisão também chamados de nós das árvores, e os nós finais onde não é possível dividir são chamados de folhas. As árvores de decisão são bastante populares no campo do aprendizado de máquina, possuindo bom desempenho e robustez a dados não escalonados, todavia ela é propensa a sobreajuste isto é, quando ela possui um desempenho satisfatório nos dados de treino, todavia não é refletido nos dados de teste. Observe abaixo as principais medidas utilizadas para calcular a homogeneidade dos dados: Algoritmo 2: Árvore de decisão binária simples Inicialize \\(D\\): \\(D = \\{(x^{(1)}, y^{(1)}),...,(x^{(n)},y^{(n)})\\} \\forall y \\in {0,1}\\) Inicialize \\(Xi\\): Melhor Atributo para divisão dos dados    GerarArvore(\\(D\\)):        Se \\(y = 1\\) ou \\(y = 0\\) \\(\\forall y \\subset D\\):            retorne Árvore        Senão:            \\(Xi = pegarMelhorAtributo(D)\\)            \\(D_{0} = DividirDados(Xi)\\)            \\(D_{1} = DividirDados(Xi)\\)        retorne \\(Nó(Xi,GerarArvore(D_{0}),GerarArvore(D_{1}))\\) Observe abaixo as medidas utilizas para identificar o melhor atributo: \\[\\begin{equation} gini = 1 - \\sum_{i = 1}^{c}(p_{i})^{2} \\tag{3} \\end{equation}\\] \\[\\begin{equation} ganho = Entropia(S) - \\sum_{v \\in A} \\frac{|S_{v}|}{|S|} . Entropia(S_{v}) \\tag{4} \\end{equation}\\] \\[\\begin{equation} entropia = - \\sum_{i = 1}^{C}(p_{i})log{_2}{(p_{i})} \\tag{5} \\end{equation}\\] Sendo: \\(p_{i}\\): Probabilidade da classe; \\(S\\): Coluna dos rótulos; \\(C\\): Número de classes distintas; \\(A\\): Elementos distintos de uma coluna; Suport Vector Machine(Máquinas de vetores de suporte) A SVM(Suport Vector Machine) é um algoritmo amplamente utilizado no aprendizado de máquina, tanto para a classificação quanto para a regressão. Seu funcionamento consistem em criar um hiperplano que melhor separe os dados ou seja, um hiperplano que maximize a margem entre os dados, para isso o algoritmo utiliza-se de dados auxiliares chamados de vetores de suporte, que ficam nos limites das margens: \\[\\begin{equation} w_{*} = \\arg \\min_{w} \\frac{1}{2} ||w||^{2} + C \\sum_{i}\\xi_{i} \\tag{6} \\end{equation}\\] As SVMs possuem duas abordagens: Soft Margin e Hard Margin. A Hard Margin nada mais é do que ajustar o hiperplano para separar os dados sem tolerar nenhum erro do modelo, isso significa que caso tenha alguma anomalia nos dados, a abordagem Hard Margin continuará ajustando o hiperplano, o que pode ocasionar em um sobreajuste. Já a soft margin tolera que o algoritmo erre em alguns pontos. Como visto na equação acima o termo \\(C \\sum_{i}\\xi_{i}\\) serve para o ponderamento em questão. Valores muito altos na constante \\(C\\) faz com que o algoritmo tolere menos os erros, já valores pequenos há uma certa tolerância. Como as SVMs trabalham apenas com dados linearmente separáveis, se faz necessárias técnica para transformação desses dados. Essas técnicas são chamadas de Kernel Tricks ou truque do Kernel. O que ele faz é elevar os nossos dados em dimensões maiores o tornando linearmente separável: Linear: \\(&lt;X,X^{`}&gt;\\) Polinomial: \\((\\gamma&lt;X,X^{`}&gt; + r)^{d}\\) RBF: \\(exp(-\\gamma||X - X^{`}||^{2})\\) Sigmoid: \\(Tanh(\\gamma&lt;X,X^{`}&gt; + r)\\) Regressão Linear A regressão linear trata-se de um algoritmo do tipo supervisionado que lida com a predição de valores contínuos. Ela basicamente descreve a relação entre uma ou mais variáveis em um conjunto de dados: \\[\\begin{equation} ŷ = \\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} + ... + \\beta_{m}X_{mi} + e_{i} \\tag{7} \\end{equation}\\] Os valores \\((\\beta_{0}, \\beta_{1}, ... ,\\beta_{m})\\) são os coeficientes utilizados para realizar o produto com cada atributo \\((X_{1},X_{2},...,X_{m})\\) sendo \\(m\\) o número de atributos e \\(n\\) o número de observações. Para encontrar o melhor estimador, utiliza-se de técnicas que visam diminuir uma função de custo, esta tem como objetivo quantificar o erro do estimador. A função de cuso mais utilizada é o Erro Quadrático Médio: \\[\\begin{equation} MSE(ŷ,y) = \\frac{1}{n} \\sum_{i}(ŷ_{i} - y_{i})^{2} \\tag{8} \\end{equation}\\] Regressão Logística A regressão logística é um tipo especial de regressão que é utilizada no paradigma de classificação. Seu funcionamento se dá baseando-se um uma função que retorna valores situados entre 0 e 1 denominada função sigmoid: \\[\\begin{equation} z = \\beta^{T} X \\tag{9} \\end{equation}\\] \\[\\begin{equation} \\sigma(z) = \\frac{1}{1+e^{-z}} \\tag{10} \\end{equation}\\] o resultado de \\(z\\) é passado para uma função \\(\\sigma\\) que retorna uma probabilidade associada. Portanto a predição de \\(ŷ\\) será: \\[\\begin{equation} ŷ = \\begin{cases} 1, \\sigma(z) \\geq 0,5 \\\\ 0, \\sigma(z) &lt; 0,5 \\end{cases} \\tag{11} \\end{equation}\\] Naive Bayes O algoritmo Naive Bayes ou também chamado de Bayes “ingênuo” é um algoritmo de aprendizado de máquina utilizado na classificação. O seu funcionamento consiste em utilizar o teorema de Thomas Bayes para a classificação de uma classe \\(y\\) dado os atributos \\((X_{1},X_{2},..., X_{m})\\). O adjetivo “ingênuo” é usado, pois deve-se assumir que os atributos são independentes já que assim evita-se um modelo complexo demais: \\[\\begin{equation} P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} \\tag{12} \\end{equation}\\] \\[\\begin{equation} P(y|X_{1},X_{2},...,X_{m}) = \\arg \\max_{j = 1...k} \\frac {\\prod P(X_{1}|y)...P(X_{m}|y) * P(y)} {P(X_{1}) ... P(X_{m})} \\tag{13} \\end{equation}\\] Modelos Ensemble Os modelos em conjunto(Ensemble) são uma família de estimadores que possui uma característica em comum, cada estimador é composto por um conjunto de estimadores. O processo consiste em agregar um conjunto de estimadores para tomar uma decisão, o que pode potencializar a capacidade de generalização do modelo. Random Forest O algoritmo Random Forest é composto por uma combinação de árvores de decisão, onde cada decisão de cada árvore impacta na classificação final,isso significa que cada árvore possui um voto para decidir qual classe escolher,sendo essa técnica denominada Voting. No caso da regressão, é computada a média dos ramos de cada árvore. O random forest utiliza a técnica de bagging em seu funcionamento, isso significa que antes de alimentar cada árvore de decisão, é feito uma reamostragem nas linhas e colunas da base de dados, sendo o processo de amostragem bootstrap a mais utilizada. Isso faz com que sejam mais robustas a sobreajuste em comparação a uma única árvore de decisão. AdaBoost O algoritmo AdaBoost utiliza a abordagem de boosting em sua composição, isso significa que ele combina um conjunto de classificadores fracos(weak learners) que juntos, criam um modelo robusto para processos de predição: Algoritmo 3: Algoritmo AdaBoost para classificação Inicialize: \\(T\\): número de classificadores fracos Inicialize \\(D\\): \\(D = \\{(x^{(1)}, y^{(1)}),...,(x^{(n)},y^{(n)})\\}\\) Inicialize \\(w_{j}(i)\\): \\(w_{j}(i) = \\frac{1}{n}\\), \\(i = 1,..,n, w \\in R^{n}\\) Para \\(j = 1,..T\\) faça:    \\(h_{j} = TreineClassificadorFraco(D,w_{j})\\)    \\(E_{j} = \\sum_{i} w_{j}(i) 1(h_{j}(x) \\neq y_{i})\\)    \\(\\alpha_{j} = \\frac{1}{2}log( \\frac{(1-E_{j})}{E_{j}})\\)    \\(w_{j+1}(i) = \\frac{w_{j}(i)}{\\sum_{i}w_{j}(i)} . a(\\alpha)\\)    \\[\\begin{equation} a(\\alpha) = \\begin{cases} e^{-\\alpha_{j}}, h_{j}(x) \\neq y_{i} \\\\ e^{\\alpha_{j}}, h_{j}(x) = y_{i} \\end{cases} \\end{equation}\\] \\(h_{final} = sign(\\sum_{i}^{T} \\alpha_{i}h_{i}(x) )\\) O funcionamento do AdaBoost consiste em criar pesos de acordo com cada linha da base de dados. O valor inicial do peso será igual para todas as linhas da base de dados: \\(w_{j}(i) = \\frac{1}{n}\\). Depois é criado um classificador que possui um desempenho um pouco superior a um classificador aleatório, esse tem o nome de aprendiz fraco(weak learner) que irá computar suas predições. Feito isso, o próximo passo será calcular \\(\\alpha_{j}\\) com base em \\(E_{j}\\) que computa as instâncias que o aprendiz fraco errou. O \\(\\alpha_{j}\\) tem como objetivo ponderar cada aprendiz fraco. Feito isso, o proximo passo consiste em atualizar cada peso \\(w_{j}(i)\\). Por fim é realizado as predições levando em consideração o parâmetro \\(\\alpha_{j}\\) em cada aprendiz fraco. Gradient Boost O algoritmo Gradient Boost é um classificador utilizado tanto na regressão, quanto na classificação. Sua ideia é utilizar a técnica de descida do gradiente para computar os resíduos de um classificador anterior, para então alimentar o próximo. Seu funcionamento se baseia na técnica de boost, usando uma sequência de classificadores sendo alimentados pelo resíduos de seus predecessores: Algoritmo 4: Algoritmo Gradient Boost com árvores de decisão para regressão Inicialize: \\(T\\): número de classificadores fracos Inicialize \\(D\\): \\(D = \\{(x^{(1)}, y^{(1)}),...,(x^{(n)},y^{(n)})\\}\\) Inicialize \\(L\\): \\(L(y^{(i)},h(x^{(i)}))\\) \\(h_{0} = \\arg \\min_{ŷ} \\sum_{i = 1}^{n} L(y^{(i)},ŷ)\\) Para \\(t = 1,...,T\\) faça:    \\(r_{i,t} = - \\frac{\\partial L(y^{(i)},h(x^{(i)}))}{\\partial h(x^{(i)})}, para\\) \\(i = 1,...,n\\), \\(h(x^{(i)}) = h_{t-1}(x^{(i)})\\)    Treinar o novo modelo com os resíduos \\(r_{i,t}\\) e criar nós \\(R_{j,t}\\) para \\(j = 1,..., J_{t}\\)    Para \\(j = 1,...,J_{t}\\) faça:        \\(ŷ_{j,t} = \\arg \\min_{ŷ} \\sum_{i = 1}^{n} L(y^{(i)},ŷ + h_{t-1}(x^{(i)}))\\)        atualizar o modelo \\(h_{t}(x) = h_{t-1}(x) + \\alpha \\sum_{j=1}^{J_{t}}ŷ_{j,t}\\) \\(I(x \\in R_{j,t})\\) Para a implementação do modelo, primeiro deve-se construir um modelo \\(h_{0}\\) base, geralmente é utilizado a média aritmética das do valor de \\(y^{(i)}\\). Feito isso, o próximo passo será computar os resíduos do modelo com o objetivo de alimentar o próximo classificador. Como se trata de uma árvore de decisão, queremos predições de cada nó que minimizem a função de custo. Por fim, é computado o somatório das predições de cada modelo. "],["métricas-para-avaliação.html", "MÉTRICAS PARA AVALIAÇÃO Subajuste e Sobreajuste Métricas para Classificação Métricas para Regressão", " MÉTRICAS PARA AVALIAÇÃO Subajuste e Sobreajuste O subajuste e sobreajuste são um dos principais conceitos que norteiam o campo do aprendizado de máquina. Estão ligados ao desempenho do modelo e sua capacidade de generalização, dado um conjunto de dados. O sobreajutes ou também chamado de variância, é a capacidade do algoritmo conseguir generalizar bem para novos conjuntos de dados, isso que significa que se tivermos um algoritmo que foi bem na etapa de treinamento, porém foi péssimo na etapa de teste, ele possui uma alta variância ou pode se dizer que ele se sobreajustou aos dados de treino. Já o subajuste, lida com a precisão do modelo. Caso um classificador não consiga um desempenho satisfatório no conjunto de dados de treino e no de teste, temos que ele está subajustado ou possui um alto viés. Métricas para Classificação Matriz de Confusão A matriz de confusão é uma métrica que cria uma matriz quadrada com um número \\(j\\) de classes. Em uma classificação binária,temos: Verdadeiros positivos: Quando o algoritmo prediz corretamente a classe positiva. Verdadeiros negativos: Quando o algoritmo prediz corretamente a classe negativa. Falsos positivos: Quando o algoritmo prediz uma instância negativa como positiva. Falsos negativos: Quando o algotitmo prediz uma instância positiva como negativa. Esses quatro conceitos serão utilizados como componentes para as próximas métrcias. Acurácia A acurácia é a medida mais comumente utilizada. Ela é basicamente a soma dos acertos do classificador dividido pelo total de instâncias: \\[\\begin{equation} Acurácia = \\frac{VP + VN}{VP+FP + VN + FN} \\tag{14} \\end{equation}\\] É preciso muito cuidado quando se utilizar essa métrica, pois torna-se bastante enviesada quando o conjunto de dados está com classes desbalanceadas. Precision(Precisão) Precision(Precisão), também chamada de acurácia das predições positivas, é a razão entre os verdadeiros positivos com a soma dos verdadeiros positivos e os falsos positivos: \\[\\begin{equation} Precison = \\frac{VP}{VP+FP} \\tag{15} \\end{equation}\\] Pela natureza da equação, valores muito baixos indicam que há uma grande presença de falsos positivos que o modelo classificou errado. Recall(Revocação) Recall(Revocação) ou taxa de verdadeiros positivos, trata-se da proporção de elementos positivos que são corretamente identificadas pelo modelo: \\[\\begin{equation} Recall = \\frac{VP}{VP+FN} \\tag{16} \\end{equation}\\] Essa métrica é mais sensível aos falsos negativos, isso significa que valores baixos indicam uma alta presença de falsos negativos que o modelo classificou errôneamente. F1 score(Pontuação F1) F1 score é a média harmônica das métricas precisão com a revocação. Ela é uma medida alternativa que serve para resumir as duas métricas: \\[\\begin{equation} F1 = 2 . \\frac{Precision . Recall}{Precision + Recall} \\tag{17} \\end{equation}\\] Valores baixos nessa métrica indicam que o modelo não esta com um desempenho satisfatório. Curva ROC A curva ROC ou curva característica de operação(receiver operating characteristic) é um gráfico usado em classificações binárias, onde o eixo \\(x\\) representam a taxa de falsos positivos e o eixo \\(y\\) representam a taxa de verdadeiros positivos ou revocação. Quanto maior a curva, melhor o classificador. Uma forma mais utilizada é computar a área sob a curva. Curva de calibração A curva de calibração é um gráfico que mostra o agrupamento das predições em forma de probabilidade no eixo \\(x\\) e a frequência do evento nos agrupamentos no eixo \\(y\\). Para um modelo ser perfeitamente calibrado, deve conter a mesma probabilidade tanto no eixo horizontal quanto no eixo vertical. Valores anômalos com probabilidade alta ou baixa define um modelo mal calibrado. Métricas para Regressão MSE(Erro quadrático Médio) O MSE é uma das funções de custo mais famosas e utilizadas no campo da regressão. É o somatório da diferença ao quadrado do rótulo verdadeiro e o rótulo predito dividido pelo total de amostras: \\[\\begin{equation} MSE = \\frac{1}{n}\\sum_{i}^{n}(y_{i} - ŷ_{i})^{2}) \\tag{18} \\end{equation}\\] RMSE(Raiz Quadrada do Erro Quadrático Médio) A métrica RMSE constitui-se da raiz do erro quadrático médio. Diferente do MSE, o RMSE é mais atenta a anomalias: \\[\\begin{equation} RMSE = \\sqrt{ \\frac{1}{n}\\sum_{i}^{n}(y_{i} - ŷ_{i})^{2}} \\tag{19} \\end{equation}\\] MAE(Erro médio absoluto) O erro médio absoluto trata-se da média do modulo da diferença entre o valor real e o valor predito: \\[\\begin{equation} MAE = \\frac{1}{n}\\sum_{i}^{n}|y_{i} - ŷ_{i}| \\tag{20} \\end{equation}\\] "],["referências.html", "REFERÊNCIAS", " REFERÊNCIAS "]]
