<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>ALGORITMOS PREDITIVOS | CAPITULO 1 - APRENDIZADO DE MÁQUINA</title>
  <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="ALGORITMOS PREDITIVOS | CAPITULO 1 - APRENDIZADO DE MÁQUINA" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="ALGORITMOS PREDITIVOS | CAPITULO 1 - APRENDIZADO DE MÁQUINA" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  

<meta name="author" content="Mateus Silva Rocha" />


<meta name="date" content="2022-06-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="passos-para-a-realização-de-um-projeto-de-aprendizado-de-máquina.html"/>
<link rel="next" href="métricas-para-avaliação.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>RESUMO</a></li>
<li class="chapter" data-level="" data-path="introdução.html"><a href="introdução.html"><i class="fa fa-check"></i>INTRODUÇÃO</a></li>
<li class="chapter" data-level="" data-path="principais-paradigmas-de-aprendizado.html"><a href="principais-paradigmas-de-aprendizado.html"><i class="fa fa-check"></i>PRINCIPAIS PARADIGMAS DE APRENDIZADO</a>
<ul>
<li class="chapter" data-level="" data-path="principais-paradigmas-de-aprendizado.html"><a href="principais-paradigmas-de-aprendizado.html#supervisionado"><i class="fa fa-check"></i>Supervisionado</a></li>
<li class="chapter" data-level="" data-path="principais-paradigmas-de-aprendizado.html"><a href="principais-paradigmas-de-aprendizado.html#não-supervisionado"><i class="fa fa-check"></i>Não supervisionado</a></li>
<li class="chapter" data-level="" data-path="principais-paradigmas-de-aprendizado.html"><a href="principais-paradigmas-de-aprendizado.html#aprendizado-por-reforço"><i class="fa fa-check"></i>Aprendizado por reforço</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="passos-para-a-realização-de-um-projeto-de-aprendizado-de-máquina.html"><a href="passos-para-a-realização-de-um-projeto-de-aprendizado-de-máquina.html"><i class="fa fa-check"></i>PASSOS PARA A REALIZAÇÃO DE UM PROJETO DE APRENDIZADO DE MÁQUINA</a>
<ul>
<li class="chapter" data-level="" data-path="passos-para-a-realização-de-um-projeto-de-aprendizado-de-máquina.html"><a href="passos-para-a-realização-de-um-projeto-de-aprendizado-de-máquina.html#entendimento-do-problema-e-coleta-de-dados"><i class="fa fa-check"></i>Entendimento do problema e coleta de dados</a></li>
<li class="chapter" data-level="" data-path="passos-para-a-realização-de-um-projeto-de-aprendizado-de-máquina.html"><a href="passos-para-a-realização-de-um-projeto-de-aprendizado-de-máquina.html#divisão-do-conjunto-de-dados"><i class="fa fa-check"></i>Divisão do conjunto de dados</a></li>
<li class="chapter" data-level="" data-path="passos-para-a-realização-de-um-projeto-de-aprendizado-de-máquina.html"><a href="passos-para-a-realização-de-um-projeto-de-aprendizado-de-máquina.html#análise-exploratória"><i class="fa fa-check"></i>Análise exploratória</a></li>
<li class="chapter" data-level="" data-path="passos-para-a-realização-de-um-projeto-de-aprendizado-de-máquina.html"><a href="passos-para-a-realização-de-um-projeto-de-aprendizado-de-máquina.html#imputação-e-transformação-dos-dados"><i class="fa fa-check"></i>Imputação e transformação dos dados</a></li>
<li class="chapter" data-level="" data-path="passos-para-a-realização-de-um-projeto-de-aprendizado-de-máquina.html"><a href="passos-para-a-realização-de-um-projeto-de-aprendizado-de-máquina.html#comparação-de-modelos"><i class="fa fa-check"></i>Comparação de modelos</a></li>
<li class="chapter" data-level="" data-path="passos-para-a-realização-de-um-projeto-de-aprendizado-de-máquina.html"><a href="passos-para-a-realização-de-um-projeto-de-aprendizado-de-máquina.html#aprimoramento-do-modelo"><i class="fa fa-check"></i>Aprimoramento do modelo</a></li>
<li class="chapter" data-level="" data-path="passos-para-a-realização-de-um-projeto-de-aprendizado-de-máquina.html"><a href="passos-para-a-realização-de-um-projeto-de-aprendizado-de-máquina.html#teste-final"><i class="fa fa-check"></i>Teste final</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="algoritmos-preditivos.html"><a href="algoritmos-preditivos.html"><i class="fa fa-check"></i>ALGORITMOS PREDITIVOS</a>
<ul>
<li class="chapter" data-level="" data-path="algoritmos-preditivos.html"><a href="algoritmos-preditivos.html#k-vizinhos-mais-próximos"><i class="fa fa-check"></i>K-vizinhos mais próximos</a></li>
<li class="chapter" data-level="" data-path="algoritmos-preditivos.html"><a href="algoritmos-preditivos.html#árvore-de-decisão"><i class="fa fa-check"></i>Árvore de decisão</a></li>
<li class="chapter" data-level="" data-path="algoritmos-preditivos.html"><a href="algoritmos-preditivos.html#suport-vector-machinemáquinas-de-vetores-de-suporte"><i class="fa fa-check"></i><em>Suport Vector Machine</em>(Máquinas de vetores de suporte)</a></li>
<li class="chapter" data-level="" data-path="algoritmos-preditivos.html"><a href="algoritmos-preditivos.html#regressão-linear"><i class="fa fa-check"></i>Regressão Linear</a></li>
<li class="chapter" data-level="" data-path="algoritmos-preditivos.html"><a href="algoritmos-preditivos.html#regressão-logística"><i class="fa fa-check"></i>Regressão Logística</a></li>
<li class="chapter" data-level="" data-path="algoritmos-preditivos.html"><a href="algoritmos-preditivos.html#naive-bayes"><i class="fa fa-check"></i><em>Naive Bayes</em></a></li>
<li class="chapter" data-level="" data-path="algoritmos-preditivos.html"><a href="algoritmos-preditivos.html#modelos-ensemble"><i class="fa fa-check"></i>Modelos <em>Ensemble</em></a>
<ul>
<li class="chapter" data-level="" data-path="algoritmos-preditivos.html"><a href="algoritmos-preditivos.html#random-forest"><i class="fa fa-check"></i><em>Random Forest</em></a></li>
<li class="chapter" data-level="" data-path="algoritmos-preditivos.html"><a href="algoritmos-preditivos.html#adaboost"><i class="fa fa-check"></i><em>AdaBoost</em></a></li>
<li class="chapter" data-level="" data-path="algoritmos-preditivos.html"><a href="algoritmos-preditivos.html#gradient-boost"><i class="fa fa-check"></i><em>Gradient Boost</em></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="métricas-para-avaliação.html"><a href="métricas-para-avaliação.html"><i class="fa fa-check"></i>MÉTRICAS PARA AVALIAÇÃO</a>
<ul>
<li class="chapter" data-level="" data-path="métricas-para-avaliação.html"><a href="métricas-para-avaliação.html#subajuste-e-sobreajuste"><i class="fa fa-check"></i>Subajuste e Sobreajuste</a></li>
<li class="chapter" data-level="" data-path="métricas-para-avaliação.html"><a href="métricas-para-avaliação.html#métricas-para-classificação"><i class="fa fa-check"></i>Métricas para Classificação</a>
<ul>
<li class="chapter" data-level="" data-path="métricas-para-avaliação.html"><a href="métricas-para-avaliação.html#matriz-de-confusão"><i class="fa fa-check"></i>Matriz de Confusão</a></li>
<li class="chapter" data-level="" data-path="métricas-para-avaliação.html"><a href="métricas-para-avaliação.html#acurácia"><i class="fa fa-check"></i>Acurácia</a></li>
<li class="chapter" data-level="" data-path="métricas-para-avaliação.html"><a href="métricas-para-avaliação.html#precisionprecisão"><i class="fa fa-check"></i><em>Precision</em>(Precisão)</a></li>
<li class="chapter" data-level="" data-path="métricas-para-avaliação.html"><a href="métricas-para-avaliação.html#recallrevocação"><i class="fa fa-check"></i><em>Recall</em>(Revocação)</a></li>
<li class="chapter" data-level="" data-path="métricas-para-avaliação.html"><a href="métricas-para-avaliação.html#f1-scorepontuação-f1"><i class="fa fa-check"></i>F1 <em>score</em>(Pontuação F1)</a></li>
<li class="chapter" data-level="" data-path="métricas-para-avaliação.html"><a href="métricas-para-avaliação.html#curva-roc"><i class="fa fa-check"></i>Curva <em>ROC</em></a></li>
<li class="chapter" data-level="" data-path="métricas-para-avaliação.html"><a href="métricas-para-avaliação.html#curva-de-calibração"><i class="fa fa-check"></i>Curva de calibração</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="métricas-para-avaliação.html"><a href="métricas-para-avaliação.html#métricas-para-regressão"><i class="fa fa-check"></i>Métricas para Regressão</a>
<ul>
<li class="chapter" data-level="" data-path="métricas-para-avaliação.html"><a href="métricas-para-avaliação.html#mseerro-quadrático-médio"><i class="fa fa-check"></i><em>MSE</em>(Erro quadrático Médio)</a></li>
<li class="chapter" data-level="" data-path="métricas-para-avaliação.html"><a href="métricas-para-avaliação.html#rmseraiz-quadrada-do-erro-quadrático-médio"><i class="fa fa-check"></i><em>RMSE</em>(Raiz Quadrada do Erro Quadrático Médio)</a></li>
<li class="chapter" data-level="" data-path="métricas-para-avaliação.html"><a href="métricas-para-avaliação.html#maeerro-médio-absoluto"><i class="fa fa-check"></i><em>MAE</em>(Erro médio absoluto)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="projeto-de-aprendizado-de-máquina-no-python.html"><a href="projeto-de-aprendizado-de-máquina-no-python.html"><i class="fa fa-check"></i>Projeto de Aprendizado de Máquina no <em>python</em></a>
<ul>
<li class="chapter" data-level="" data-path="projeto-de-aprendizado-de-máquina-no-python.html"><a href="projeto-de-aprendizado-de-máquina-no-python.html#entendimento-do-problema"><i class="fa fa-check"></i>Entendimento do problema</a></li>
<li class="chapter" data-level="" data-path="projeto-de-aprendizado-de-máquina-no-python.html"><a href="projeto-de-aprendizado-de-máquina-no-python.html#bibliotecas-utilizadas"><i class="fa fa-check"></i>Bibliotecas utilizadas</a></li>
<li class="chapter" data-level="" data-path="projeto-de-aprendizado-de-máquina-no-python.html"><a href="projeto-de-aprendizado-de-máquina-no-python.html#coleta-de-dados"><i class="fa fa-check"></i>Coleta de dados</a></li>
<li class="chapter" data-level="" data-path="projeto-de-aprendizado-de-máquina-no-python.html"><a href="projeto-de-aprendizado-de-máquina-no-python.html#divisão-dos-dados"><i class="fa fa-check"></i>Divisão dos dados</a></li>
<li class="chapter" data-level="" data-path="projeto-de-aprendizado-de-máquina-no-python.html"><a href="projeto-de-aprendizado-de-máquina-no-python.html#análise-exploratória-1"><i class="fa fa-check"></i>Análise exploratória</a></li>
<li class="chapter" data-level="" data-path="projeto-de-aprendizado-de-máquina-no-python.html"><a href="projeto-de-aprendizado-de-máquina-no-python.html#imputação-e-transformação-dos-dados-1"><i class="fa fa-check"></i>Imputação e transformação dos dados</a></li>
<li class="chapter" data-level="" data-path="projeto-de-aprendizado-de-máquina-no-python.html"><a href="projeto-de-aprendizado-de-máquina-no-python.html#comparação-entre-modelos"><i class="fa fa-check"></i>Comparação entre modelos</a></li>
<li class="chapter" data-level="" data-path="projeto-de-aprendizado-de-máquina-no-python.html"><a href="projeto-de-aprendizado-de-máquina-no-python.html#aprimoramento-do-modelo-1"><i class="fa fa-check"></i>Aprimoramento do modelo</a></li>
<li class="chapter" data-level="" data-path="projeto-de-aprendizado-de-máquina-no-python.html"><a href="projeto-de-aprendizado-de-máquina-no-python.html#teste-final-1"><i class="fa fa-check"></i>Teste final</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="projeto-de-aprendizado-de-máquina-no-r.html"><a href="projeto-de-aprendizado-de-máquina-no-r.html"><i class="fa fa-check"></i>Projeto de Aprendizado de Máquina no R</a>
<ul>
<li class="chapter" data-level="" data-path="projeto-de-aprendizado-de-máquina-no-r.html"><a href="projeto-de-aprendizado-de-máquina-no-r.html#entendimento-do-problema-1"><i class="fa fa-check"></i>Entendimento do problema</a></li>
<li class="chapter" data-level="" data-path="projeto-de-aprendizado-de-máquina-no-r.html"><a href="projeto-de-aprendizado-de-máquina-no-r.html#bibliotecas-utilizadas-1"><i class="fa fa-check"></i>Bibliotecas utilizadas</a></li>
<li class="chapter" data-level="" data-path="projeto-de-aprendizado-de-máquina-no-r.html"><a href="projeto-de-aprendizado-de-máquina-no-r.html#coleta-de-dados-1"><i class="fa fa-check"></i>Coleta de dados</a></li>
<li class="chapter" data-level="" data-path="projeto-de-aprendizado-de-máquina-no-r.html"><a href="projeto-de-aprendizado-de-máquina-no-r.html#divisão-dos-dados-1"><i class="fa fa-check"></i>Divisão dos dados</a></li>
<li class="chapter" data-level="" data-path="projeto-de-aprendizado-de-máquina-no-r.html"><a href="projeto-de-aprendizado-de-máquina-no-r.html#análise-exploratória-2"><i class="fa fa-check"></i>Análise exploratória</a></li>
<li class="chapter" data-level="" data-path="projeto-de-aprendizado-de-máquina-no-r.html"><a href="projeto-de-aprendizado-de-máquina-no-r.html#imputação-e-transformação-dos-dados-2"><i class="fa fa-check"></i>Imputação e transformação dos dados</a></li>
<li class="chapter" data-level="" data-path="projeto-de-aprendizado-de-máquina-no-r.html"><a href="projeto-de-aprendizado-de-máquina-no-r.html#comparação-entre-modelos-1"><i class="fa fa-check"></i>Comparação entre modelos</a></li>
<li class="chapter" data-level="" data-path="projeto-de-aprendizado-de-máquina-no-r.html"><a href="projeto-de-aprendizado-de-máquina-no-r.html#aprimoramento-do-modelo-2"><i class="fa fa-check"></i>Aprimoramento do modelo</a></li>
<li class="chapter" data-level="" data-path="projeto-de-aprendizado-de-máquina-no-r.html"><a href="projeto-de-aprendizado-de-máquina-no-r.html#teste-final-2"><i class="fa fa-check"></i>Teste final</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="referências.html"><a href="referências.html"><i class="fa fa-check"></i>Referências</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">CAPITULO 1 - APRENDIZADO DE MÁQUINA</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="algoritmos-preditivos" class="section level1 unnumbered hasAnchor">
<h1>ALGORITMOS PREDITIVOS<a href="algoritmos-preditivos.html#algoritmos-preditivos" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="k-vizinhos-mais-próximos" class="section level2 unnumbered hasAnchor">
<h2>K-vizinhos mais próximos<a href="algoritmos-preditivos.html#k-vizinhos-mais-próximos" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>O algoritmo de K-vizinhos mais próximos ou KNN(<em>k-nearest neighbors</em>) é um tipo
de algoritmo de aprendizado de máquina baseado em exemplos ou instâncias. Isso
significa que, invés de estimar algum parâmetro para realizar predições cada
vez mais precisas, ele realiza um processo de “memorização” baseado em algum
critério de similaridade entre as instâncias do conjunto de dados<span class="citation">(Moraes Batista and Chiavegatto Filho 2019)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-6"></span>
<img src="figuras/knn.png" alt="Ilustração do algoritmo KNN" width="710" />
<p class="caption">
Figure 6: Ilustração do algoritmo KNN
</p>
</div>
<p>O seu funcionamento consiste em escolher um número K de vizinhos mais próximos
para realizar a predição.Definido a quantidade, o algoritmo irá percorrer todo
o conjunto de dados procurando os k mais próximos. Por fim, caso o algoritmo
seja usado para classificação, irá ser feito pela classe mais frequente, ou seja
voto majoritário, caso seja usado para regressão, será computado a média dos k
vizinhos mais próximos:</p>
<hr />
<p><strong>Algoritmo 1</strong>: K vizinhos mais próximos para <span class="math inline">\(K = 1\)</span></p>
<hr />
<div class="line-block"><strong>Inicialize</strong> <span class="math inline">\(D\)</span>: <span class="math inline">\(D = \{(x^{(1)}, y^{(1)}),...,(x^{(n)},y^{(n)})\}\)</span></div>
<div class="line-block"><strong>Inicialize</strong> <span class="math inline">\(x^{(P)}\)</span>: Sendo <span class="math inline">\(x^{(P)}\)</span> como a instância para predição.</div>
<div class="line-block"><strong>Inicialize</strong> <span class="math inline">\(d\)</span> : <span class="math inline">\(d(x^{(i)},x^{(p)})\)</span> como a medida de similaridade.</div>
<div class="line-block">instancia_perto = <span class="math inline">\(none\)</span></div>
<div class="line-block">distancia_mais_proxima = <span class="math inline">\(\infty\)</span></div>
<div class="line-block">rotulo_perto = <span class="math inline">\(none\)</span></div>
<div class="line-block"><strong>Para</strong> <span class="math inline">\(i = 1,...,n\)</span> <strong>faça</strong>:</div>
<div class="line-block">   distancia = <span class="math inline">\(d(x^{(i)},x^{(p)})\)</span></div>
<div class="line-block">   <strong>Se</strong> distancia <span class="math inline">\(\leq\)</span> distancia_mais_proxima <strong>então</strong>:<br />
       distancia_mais_proxima = distancia<br />
       instancia_perto = <span class="math inline">\(x^{(i)}\)</span></div>
<div class="line-block">       rotulo_perto = <span class="math inline">\(y^{(i)}\)</span></div>
<div class="line-block"><strong>retorne</strong> rotulo_perto</div>
<hr />
<p>Observe abaixo exemplos de medidas de similaridade comumente utilizas no algoritmo:</p>
<p><span class="math display" id="eq:euclidiana">\[\begin{equation}
d_{E}(\vec{x}, \vec{y}) = \sqrt{\sum_{i = 1}^{n}(\vec{x_{i}} -\vec{y_{i}})^{2}}
\tag{1}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:manhattan">\[\begin{equation}
d_{M}(\vec{x}, \vec{y}) = \sum_{i = 1}^{n} | \vec{x_{i}} - \vec{y_{i}}|
\tag{2}
\end{equation}\]</span></p>
</div>
<div id="árvore-de-decisão" class="section level2 unnumbered hasAnchor">
<h2>Árvore de decisão<a href="algoritmos-preditivos.html#árvore-de-decisão" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A árvore de decisão é um método de aprendizado simbólico que é usado tanto para
a classificação quanto para a regressão.</p>
<p>O funcionamento de uma árvore de decisão em um problema de classificação
consiste em criar partições em seu conjunto de dados de modo que deixe os
dados homogêneos em relação a seus rótulos. Primeiro defini-se atributos que
sirvam como critérios para divisão também chamados de nós das árvores, e os nós
finais onde não é possível dividir são chamados de nós de folhas<span class="citation">(Quinlan 1986)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-7"></span>
<img src="figuras/arvore_decisao.png" alt="Ilustração de uma árvore de decisão" width="648" />
<p class="caption">
Figure 7: Ilustração de uma árvore de decisão
</p>
</div>
<p>As árvores de decisão são bastante populares no campo do aprendizado de máquina,
possuindo bom desempenho e robustez a dados não escalonados, todavia ela é propensa
a sobreajuste isto é, quando ela possui um desempenho satisfatório nos dados de
treino, todavia não é refletido nos dados de teste. Observe abaixo as principais
medidas utilizadas para calcular a homogeneidade dos dados:</p>
<hr />
<p><strong>Algoritmo 2</strong>: Árvore de decisão binária simples</p>
<hr />
<div class="line-block"><strong>Inicialize</strong> <span class="math inline">\(D\)</span>: <span class="math inline">\(D = \{(x^{(1)}, y^{(1)}),...,(x^{(n)},y^{(n)})\} \forall y \in {0,1}\)</span></div>
<div class="line-block"><strong>Inicialize</strong> <span class="math inline">\(Xi\)</span>: Melhor Atributo para divisão dos dados</div>
<div class="line-block">   GerarArvore(<span class="math inline">\(D\)</span>):</div>
<div class="line-block">       Se <span class="math inline">\(y = 1\)</span> ou <span class="math inline">\(y = 0\)</span> <span class="math inline">\(\forall y \subset D\)</span>:</div>
<div class="line-block">           <strong>retorne</strong> Árvore</div>
<div class="line-block">       Senão:</div>
<div class="line-block">           <span class="math inline">\(Xi = pegarMelhorAtributo(D)\)</span></div>
<div class="line-block">           <span class="math inline">\(D_{0} = DividirDados(Xi)\)</span></div>
<div class="line-block">           <span class="math inline">\(D_{1} = DividirDados(Xi)\)</span></div>
<div class="line-block">       <strong>retorne</strong> <span class="math inline">\(Nó(Xi,GerarArvore(D_{0}),GerarArvore(D_{1}))\)</span></div>
<hr />
<p>Observe abaixo as medidas utilizas para identificar o melhor atributo:</p>
<p><span class="math display" id="eq:gini">\[\begin{equation}
gini = 1 - \sum_{i = 1}^{c}(p_{i})^{2}
\tag{3}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:ganho">\[\begin{equation}
ganho = Entropia(S) - \sum_{v \in A} \frac{|S_{v}|}{|S|} . Entropia(S_{v})
\tag{4}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:entropia">\[\begin{equation}
entropia = - \sum_{i = 1}^{C}(p_{i})log{_2}{(p_{i})}
\tag{5}
\end{equation}\]</span></p>
<p>Sendo:</p>
<ul>
<li><span class="math inline">\(p_{i}\)</span>: Probabilidade da classe;</li>
<li><span class="math inline">\(S\)</span>: Coluna dos rótulos;</li>
<li><span class="math inline">\(C\)</span>: Número de classes distintas;</li>
<li><span class="math inline">\(A\)</span>: Elementos distintos de uma coluna;</li>
</ul>
</div>
<div id="suport-vector-machinemáquinas-de-vetores-de-suporte" class="section level2 unnumbered hasAnchor">
<h2><em>Suport Vector Machine</em>(Máquinas de vetores de suporte)<a href="algoritmos-preditivos.html#suport-vector-machinemáquinas-de-vetores-de-suporte" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A SVM(<em>Suport Vector Machine</em>) é um algoritmo amplamente utilizado no aprendizado
de máquina, tanto para a classificação quanto para a regressão. Seu funcionamento
consistem em criar um hiperplano que melhor separe os dados ou seja, um hiperplano
que maximize a margem entre os dados, para isso o algoritmo utiliza-se de dados
auxiliares chamados de vetores de suporte, que ficam nos limites das margens<span class="citation">(Boser, Guyon, and Vapnik 1992)</span>:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-8"></span>
<img src="figuras/svm.png" alt="Ilustração de uma máquina de vetores de suporte" width="595" />
<p class="caption">
Figure 8: Ilustração de uma máquina de vetores de suporte
</p>
</div>
<p><span class="math display" id="eq:svm">\[\begin{equation}
w_{*} = \arg \min_{w} \frac{1}{2} ||w||^{2} + C \sum_{i}\xi_{i}
\tag{6}
\end{equation}\]</span></p>
<p>As SVMs possuem duas abordagens: <em>Soft Margin</em> e <em>Hard Margin</em>. A <em>Hard Margin</em>
nada mais é do que ajustar o hiperplano para separar os dados sem tolerar nenhum
erro do modelo, isso significa que caso tenha alguma anomalia nos dados, a abordagem
<em>Hard Margin</em> continuará ajustando o hiperplano, o que pode ocasionar em um sobreajuste.
Já a <em>soft margin</em> tolera que o algoritmo erre em alguns pontos. Como visto na
equação acima o termo <span class="math inline">\(C \sum_{i}\xi_{i}\)</span> serve para o ponderamento em questão.
Valores muito altos na constante <span class="math inline">\(C\)</span> faz com que o algoritmo tolere menos os
erros, já valores pequenos há uma certa tolerância.</p>
<p>Como as SVMs trabalham apenas com dados linearmente separáveis, se faz necessárias
técnica para transformação desses dados. Essas técnicas são chamadas de <em>Kernel Tricks</em>
ou truque do <em>Kernel</em>. O que ele faz é elevar os nossos dados em dimensões maiores
o tornando linearmente separável:</p>
<ul>
<li><p>Linear: <span class="math inline">\(&lt;X,X^{`}&gt;\)</span></p></li>
<li><p>Polinomial: <span class="math inline">\((\gamma&lt;X,X^{`}&gt; + r)^{d}\)</span></p></li>
<li><p>RBF: <span class="math inline">\(exp(-\gamma||X - X^{`}||^{2})\)</span></p></li>
<li><p>Sigmoid: <span class="math inline">\(Tanh(\gamma&lt;X,X^{`}&gt; + r)\)</span></p></li>
</ul>
</div>
<div id="regressão-linear" class="section level2 unnumbered hasAnchor">
<h2>Regressão Linear<a href="algoritmos-preditivos.html#regressão-linear" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A regressão linear trata-se de um algoritmo do tipo supervisionado que lida com
a predição de valores contínuos. Ela basicamente descreve a relação entre uma ou
mais variáveis em um conjunto de dados:</p>
<p><span class="math display" id="eq:reg">\[\begin{equation}
ŷ = \beta_{0} + \beta_{1}X_{i1} + \beta_{2}X_{i2} + ... + \beta_{m}X_{mi} + e_{i}
\tag{7}
\end{equation}\]</span>
Os valores <span class="math inline">\((\beta_{0}, \beta_{1}, ... ,\beta_{m})\)</span> são os coeficientes utilizados
para realizar o produto com cada atributo <span class="math inline">\((X_{1},X_{2},...,X_{m})\)</span> sendo <span class="math inline">\(m\)</span> o
número de atributos e <span class="math inline">\(n\)</span> o número de observações. Para encontrar o melhor estimador,
utiliza-se de técnicas que visam diminuir uma função de custo, esta tem como
objetivo quantificar o erro do estimador. A função de cuso mais utilizada é o
Erro Quadrático Médio:</p>
<p><span class="math display" id="eq:erro">\[\begin{equation}
MSE(ŷ,y) = \frac{1}{n} \sum_{i}(ŷ_{i} - y_{i})^{2}
\tag{8}
\end{equation}\]</span></p>
</div>
<div id="regressão-logística" class="section level2 unnumbered hasAnchor">
<h2>Regressão Logística<a href="algoritmos-preditivos.html#regressão-logística" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A regressão logística é um tipo especial de regressão que é utilizada no paradigma de
classificação. Seu funcionamento se dá baseando-se um uma função que retorna valores
situados entre 0 e 1 denominada função sigmoid<span class="citation">(Moraes Batista and Chiavegatto Filho 2019)</span>:</p>
<p><span class="math display" id="eq:funcaoz">\[\begin{equation}
z = \beta^{T} X
\tag{9}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:sigma">\[\begin{equation}
\sigma(z) = \frac{1}{1+e^{-z}}
\tag{10}
\end{equation}\]</span></p>
<p>o resultado de <span class="math inline">\(z\)</span> é passado para uma função <span class="math inline">\(\sigma\)</span> que retorna uma probabilidade
associada. Portanto a predição de <span class="math inline">\(ŷ\)</span> será:</p>
<p><span class="math display" id="eq:pred">\[\begin{equation}
ŷ =
  \begin{cases}
      1, \sigma(z) \geq 0,5 \\
      0, \sigma(z) &lt; 0,5
  \end{cases}
\tag{11}
\end{equation}\]</span></p>
</div>
<div id="naive-bayes" class="section level2 unnumbered hasAnchor">
<h2><em>Naive Bayes</em><a href="algoritmos-preditivos.html#naive-bayes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>O algoritmo <em>Naive Bayes</em> ou também chamado de Bayes “ingênuo” é um algoritmo
de aprendizado de máquina utilizado na classificação. O seu funcionamento consiste
em utilizar o teorema de Thomas Bayes para a classificação de uma classe <span class="math inline">\(y\)</span> dado
os atributos <span class="math inline">\((X_{1},X_{2},..., X_{m})\)</span>. O adjetivo “ingênuo” é usado, pois deve-se
assumir que os atributos são independentes já que assim evita-se um modelo complexo
demais:</p>
<p><span class="math display" id="eq:bayes">\[\begin{equation}
P(A|B) = \frac{P(B|A) * P(A)}{P(B)}
\tag{12}
\end{equation}\]</span>
<span class="math display" id="eq:naive">\[\begin{equation}
P(y|X_{1},X_{2},...,X_{m}) = \arg \max_{j = 1...k} \frac {\prod P(X_{1}|y)...P(X_{m}|y) * P(y)} {P(X_{1}) ... P(X_{m})}
\tag{13}
\end{equation}\]</span></p>
</div>
<div id="modelos-ensemble" class="section level2 unnumbered hasAnchor">
<h2>Modelos <em>Ensemble</em><a href="algoritmos-preditivos.html#modelos-ensemble" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Os modelos em conjunto(<em>Ensemble</em>) são uma família de estimadores que possui
uma característica em comum, cada estimador é composto por um conjunto de estimadores. O processo
consiste em agregar um conjunto de estimadores para tomar uma decisão, o que pode
potencializar a capacidade de generalização do modelo.</p>
<div id="random-forest" class="section level3 unnumbered hasAnchor">
<h3><em>Random Forest</em><a href="algoritmos-preditivos.html#random-forest" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>O algoritmo <em>Random Forest</em> é composto por uma combinação de árvores de decisão,
onde cada decisão de cada árvore impacta na classificação final,isso significa que cada
árvore possui um voto para decidir qual classe escolher,sendo essa técnica denominada <em>voting</em>.
No caso da regressão, é computada a média dos ramos de cada árvore<span class="citation">(Ho 1995)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-9"></span>
<img src="figuras/random_forest.png" alt="Ilustração de uma Random Forest" width="708" />
<p class="caption">
Figure 9: Ilustração de uma Random Forest
</p>
</div>
<p>O <em>random forest</em> utiliza a técnica de <em>bagging</em> em seu funcionamento, isso significa
que antes de alimentar cada árvore de decisão, é feito uma reamostragem nas linhas
e colunas da base de dados, sendo o processo de amostragem <em>bootstrap</em> a mais utilizada. Isso
faz com que sejam mais robustas a sobreajuste em comparação a uma única árvore de decisão.</p>
</div>
<div id="adaboost" class="section level3 unnumbered hasAnchor">
<h3><em>AdaBoost</em><a href="algoritmos-preditivos.html#adaboost" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>O algoritmo <em>AdaBoost</em> utiliza a abordagem de <em>boosting</em> em sua composição, isso
significa que ele combina um conjunto de classificadores fracos(<em>weak learners</em>)
que juntos, criam um modelo robusto para processos de predição<span class="citation">(Freund, Schapire, and Abe 1999)</span>:</p>
<hr />
<p><strong>Algoritmo 3</strong>: Algoritmo <em>AdaBoost</em> para classificação</p>
<hr />
<div class="line-block"><strong>Inicialize:</strong> <span class="math inline">\(T\)</span>: número de classificadores fracos</div>
<div class="line-block"><strong>Inicialize</strong> <span class="math inline">\(D\)</span>: <span class="math inline">\(D = \{(x^{(1)}, y^{(1)}),...,(x^{(n)},y^{(n)})\}\)</span></div>
<div class="line-block"><strong>Inicialize</strong> <span class="math inline">\(w_{j}(i)\)</span>: <span class="math inline">\(w_{j}(i) = \frac{1}{n}\)</span>, <span class="math inline">\(i = 1,..,n, w \in R^{n}\)</span></div>
<div class="line-block"><strong>Para</strong> <span class="math inline">\(j = 1,..T\)</span> <strong>faça</strong>:</div>
<div class="line-block">   <span class="math inline">\(h_{j} = TreineClassificadorFraco(D,w_{j})\)</span></div>
<div class="line-block">   <span class="math inline">\(E_{j} = \sum_{i} w_{j}(i) 1(h_{j}(x) \neq y_{i})\)</span></div>
<div class="line-block">   <span class="math inline">\(\alpha_{j} = \frac{1}{2}log( \frac{(1-E_{j})}{E_{j}})\)</span></div>
<div class="line-block">   <span class="math inline">\(w_{j+1}(i) = \frac{w_{j}(i)}{\sum_{i}w_{j}(i)} . a(\alpha)\)</span></div>
<div class="line-block">   <span class="math display">\[\begin{equation}      a(\alpha) =       \begin{cases}      e^{-\alpha_{j}},  h_{j}(x) \neq y_{i} \\      e^{\alpha_{j}}, h_{j}(x) = y_{i}       \end{cases}      \end{equation}\]</span><br />
<span class="math inline">\(h_{final} = sign(\sum_{i}^{T} \alpha_{i}h_{i}(x) )\)</span></div>
<hr />
<p>O funcionamento do <em>AdaBoost</em> consiste em criar pesos de acordo com cada linha
da base de dados. O valor inicial do peso será igual para todas as linhas da
base de dados: <span class="math inline">\(w_{j}(i) = \frac{1}{n}\)</span>. Depois é criado um classificador
que possui um desempenho um pouco superior a um classificador aleatório, esse
tem o nome de aprendiz fraco(<em>weak learner</em>) que irá computar suas predições.
Feito isso, o próximo passo será calcular <span class="math inline">\(\alpha_{j}\)</span> com base em <span class="math inline">\(E_{j}\)</span> que computa
as instâncias que o aprendiz fraco errou. O <span class="math inline">\(\alpha_{j}\)</span> tem como objetivo ponderar
cada aprendiz fraco. Feito isso, o proximo passo consiste em atualizar cada peso
<span class="math inline">\(w_{j}(i)\)</span>. Por fim é realizado as predições levando em consideração o parâmetro
<span class="math inline">\(\alpha_{j}\)</span> em cada aprendiz fraco.</p>
</div>
<div id="gradient-boost" class="section level3 unnumbered hasAnchor">
<h3><em>Gradient Boost</em><a href="algoritmos-preditivos.html#gradient-boost" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>O algoritmo <em>Gradient Boost</em> é um classificador utilizado tanto na regressão, quanto
na classificação. Sua ideia é utilizar a técnica de descida do gradiente para
computar os resíduos de um classificador anterior, para então alimentar o próximo.
Seu funcionamento se baseia na técnica de <em>boost</em>, usando uma sequência de classificadores
sendo alimentados pelo resíduos de seus predecessores<span class="citation">(Friedman 2001)</span>:</p>
<hr />
<p><strong>Algoritmo 4</strong>: Algoritmo <em>Gradient Boost</em> com árvores de decisão para regressão</p>
<hr />
<div class="line-block"><strong>Inicialize:</strong> <span class="math inline">\(T\)</span>: número de classificadores fracos</div>
<div class="line-block"><strong>Inicialize</strong> <span class="math inline">\(D\)</span>: <span class="math inline">\(D = \{(x^{(1)}, y^{(1)}),...,(x^{(n)},y^{(n)})\}\)</span></div>
<div class="line-block"><strong>Inicialize</strong> <span class="math inline">\(L\)</span>: <span class="math inline">\(L(y^{(i)},h(x^{(i)}))\)</span></div>
<div class="line-block"><span class="math inline">\(h_{0} = \arg \min_{ŷ} \sum_{i = 1}^{n} L(y^{(i)},ŷ)\)</span></div>
<div class="line-block"><strong>Para</strong> <span class="math inline">\(t = 1,...,T\)</span> <strong>faça</strong>:</div>
<div class="line-block">   <span class="math inline">\(r_{i,t} = - \frac{\partial L(y^{(i)},h(x^{(i)}))}{\partial h(x^{(i)})}, para\)</span> <span class="math inline">\(i = 1,...,n\)</span>, <span class="math inline">\(h(x^{(i)}) = h_{t-1}(x^{(i)})\)</span></div>
<div class="line-block">   Treinar o novo modelo com os resíduos <span class="math inline">\(r_{i,t}\)</span> e criar nós <span class="math inline">\(R_{j,t}\)</span> para <span class="math inline">\(j = 1,..., J_{t}\)</span></div>
<div class="line-block">   <strong>Para</strong> <span class="math inline">\(j = 1,...,J_{t}\)</span> <strong>faça</strong>:</div>
<div class="line-block">       <span class="math inline">\(ŷ_{j,t} = \arg \min_{ŷ} \sum_{i = 1}^{n} L(y^{(i)},ŷ + h_{t-1}(x^{(i)}))\)</span></div>
<div class="line-block">       atualizar o modelo <span class="math inline">\(h_{t}(x) = h_{t-1}(x) + \alpha \sum_{j=1}^{J_{t}}ŷ_{j,t}\)</span> <span class="math inline">\(I(x \in R_{j,t})\)</span></div>
<hr />
<p>Para a implementação do modelo, primeiro deve-se construir um modelo <span class="math inline">\(h_{0}\)</span> base, geralmente
é utilizado a média aritmética das do valor de <span class="math inline">\(y^{(i)}\)</span>. Feito isso, o próximo passo será computar
os resíduos do modelo com o objetivo de alimentar o próximo classificador. Como se trata de uma árvore
de decisão, queremos predições de cada nó que minimizem a função de custo. Por fim, é computado o somatório
das predições de cada modelo.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="passos-para-a-realização-de-um-projeto-de-aprendizado-de-máquina.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="métricas-para-avaliação.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/04-algoritmos.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
